{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deconvolution GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe \n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization, Reshape, UpSampling2D, MaxPool2D, Flatten\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Define Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()        \n",
    "        self.dense1 = Dense(1024, activation='tanh', input_dim=(100))\n",
    "        self.dense2 = Dense(128*7*7, activation='tanh')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.reshape1 = Reshape((7,7,128),input_shape=(128*7*7,))\n",
    "        self.up1 = UpSampling2D(size=(2,2))\n",
    "        self.conv1 = Conv2D(64,(5,5), padding='same', activation='tanh')\n",
    "        self.up2 = UpSampling2D(size=(2,2))\n",
    "        self.conv2 = Conv2D(1,(5,5), padding='same', activation='tanh')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.dense1(x)        \n",
    "        x = self.dense2(x)                \n",
    "        x = self.bn1(x)        \n",
    "        x = self.reshape1(x)        \n",
    "        x = self.up1(x)        \n",
    "        x = self.conv1(x)        \n",
    "        x = self.up2(x)        \n",
    "        x = self.conv2(x)        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "(1, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "test = tf.random.normal((1,100))\n",
    "print(test.shape)\n",
    "g = Generator()\n",
    "result = g(test[0:])\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Discrimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discrimator(Model):\n",
    "    def __init__(self):\n",
    "        super(Discrimator,self).__init__()\n",
    "        self.conv1 = Conv2D(64, (5,5), padding='same', activation='tanh')\n",
    "        self.maxPool1 = MaxPool2D(pool_size=(2,2))\n",
    "        self.conv2 = Conv2D(128, (5,5), padding='same', activation='tanh')\n",
    "        self.maxPool2 = MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(1024, activation='tanh')\n",
    "        self.dense2 = Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxPool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxPool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "test = tf.random.normal((1,28,28,1))\n",
    "print(test.shape)\n",
    "d = Discrimator()\n",
    "result = d(test)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Z (1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "LATENT_SIZE=100\n",
    "z = tf.random.uniform(minval=-1,maxval=1,shape=(BATCH_SIZE, LATENT_SIZE), seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100)\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, g, d):\n",
    "        self.G = g\n",
    "        self.D = d\n",
    "        self.noise_size = 100 \n",
    "        \n",
    "    def set_images(self, img_data):\n",
    "        self.img_data = img_data\n",
    "        \n",
    "    def train(self,batch_size=100):\n",
    "        # Pick image data randomly. \n",
    "        images_train = self.img_data[np.random.randint(0, self.img_data.shape[0], size=batch_size), :, :, :] \n",
    "        # Generate images from noise. \n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, self.noise_size]) \n",
    "        images_fake = self.G(noise) \n",
    "        # Train D. \n",
    "        x = np.concatenate((images_train, images_fake)) \n",
    "        y = np.ones([2*batch_size, 1]) \n",
    "        y[batch_size:, :] = 0 \n",
    "        self.D.trainable = True \n",
    "        y_hat = seld.D(x)\n",
    "        d_loss = y-y_hat\n",
    "        # Train G. \n",
    "        y = np.ones([batch_size, 1]) \n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, self.noise_size]) \n",
    "        self.D.trainable = False \n",
    "        a_loss = self.AM.train_on_batch(noise, y) \n",
    "        return d_loss, a_loss, images_fake        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-560ecb9f15ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtotal_a_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_d_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtotal_a_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ma_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2d6b4669efed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Train G.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test)  = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0 \n",
    "x_train = x_train.reshape((x_train.shape[0],) + (28, 28, 1)) \n",
    "print(x_train.shape) \n",
    "# Init network \n",
    "gan = GAN(g, d)\n",
    "gan.set_images(x_train)\n",
    "# Some parameters. \n",
    "epochs = 30 \n",
    "sample_size = 10 \n",
    "batch_size = 100 \n",
    "train_per_epoch = x_train.shape[0] // batch_size \n",
    "\n",
    "for epoch in range(0, epochs): \n",
    "    print(\"Epoch:\", epoch + 1) \n",
    "    total_d_loss = 0.0 \n",
    "    total_a_loss = 0.0 \n",
    "    for batch in range(0, train_per_epoch): \n",
    "        d_loss, a_loss, imgs = gan.train(batch_size) \n",
    "        total_d_loss += d_loss \n",
    "        total_a_loss += a_loss \n",
    "        total_d_loss /= train_per_epoch \n",
    "        total_a_loss /= train_per_epoch \n",
    "        print(\"D Loss: {}, AM Loss: {}\".format(total_d_loss, total_a_loss)) \n",
    "        # Show generated images. \n",
    "        fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1)) \n",
    "        for i in range(0, sample_size): \n",
    "            ax[i].set_axis_off() \n",
    "            ax[i].imshow(imgs[i].reshape((28, 28))); \n",
    "            plt.show() \n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
