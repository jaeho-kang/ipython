{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Mode Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist image : 28,28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    # Build the model\n",
    "    model.add(tf.keras.layers.Conv2D(16,[3,3], activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(16,[3,3], activation='relu'))\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model2():\n",
    "    model = tf.keras.Sequential()\n",
    "    # Build the model\n",
    "    model.add(tf.keras.layers.Conv2D(16,[3,3], activation='relu', use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(16,[3,3], activation='relu', use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f00c79f7668>\n"
     ]
    }
   ],
   "source": [
    "model = create_model2()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and format the mnist data\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\n",
    "   tf.cast(mnist_labels,tf.int64)))\n",
    "dataset = dataset.shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n",
      "Logis tf.Tensor(\n",
      "[[-0.00029835 -0.01244704 -0.01164015  0.03639723 -0.01314143  0.01852584\n",
      "   0.03063662 -0.00100079 -0.00109658  0.00548179]], shape=(1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in dataset.take(1):\n",
    "    print(images[0:1].shape)\n",
    "    print(\"Logis\", model(images[0:1].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "Logis tf.Tensor(\n",
      "[[ 0.00013027 -0.02444366 -0.01749717  0.06662767 -0.02093653  0.02935508\n",
      "   0.05254671 -0.00591404  0.00333205  0.00238367]], shape=(1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in dataset.take(1):\n",
    "    print(images[0].shape)\n",
    "    print(\"Logis\", model(images[0:1].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss_history = []\n",
    "# for ( batch, (images, labels)) in enumerate(dataset.take(400)):\n",
    "#     if batch % 80== 0 :\n",
    "#         print()\n",
    "#     print('.', end='')\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         logits = model(images, training=True)\n",
    "#         loss_value = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
    "        \n",
    "#     loss_history.append(loss_value.numpy())\n",
    "#     grads = tape.gradient(loss_value, model.variables)\n",
    "#     optimizer.apply_gradients(zip(grads, model.variables), \n",
    "#                              global_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(loss_history)\n",
    "# plt.xlabel('Batch #')\n",
    "# plt.ylabel('Loss [entropy]')\n",
    "# plt.show()\n",
    "# #Text(0, 0.5, 'Loss [entropy]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(model, inputs, targets):\n",
    "    error = model(inputs) - targets\n",
    "    return tf.resuce_mean(tf.square(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 000: 2.277, Accu : 0.062\n",
      "Loss at step 020: 2.067, Accu : 0.281\n",
      "Loss at step 040: 2.005, Accu : 0.250\n",
      "Loss at step 060: 1.947, Accu : 0.375\n",
      "Loss at step 080: 1.898, Accu : 0.312\n",
      "Loss at step 100: 1.868, Accu : 0.375\n",
      "Loss at step 120: 2.061, Accu : 0.250\n",
      "Loss at step 140: 1.863, Accu : 0.281\n",
      "Loss at step 160: 1.870, Accu : 0.344\n",
      "Loss at step 180: 1.838, Accu : 0.406\n",
      "Loss at step 200: 1.737, Accu : 0.344\n",
      "Loss at step 220: 1.694, Accu : 0.344\n",
      "Loss at step 240: 1.772, Accu : 0.406\n",
      "Loss at step 260: 1.752, Accu : 0.469\n",
      "Loss at step 280: 1.724, Accu : 0.469\n",
      "Loss at step 300: 1.625, Accu : 0.469\n",
      "Loss at step 320: 1.728, Accu : 0.469\n",
      "Loss at step 340: 1.542, Accu : 0.594\n",
      "Loss at step 360: 1.646, Accu : 0.406\n",
      "Loss at step 380: 1.512, Accu : 0.531\n"
     ]
    }
   ],
   "source": [
    "epoch=2\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "for _ in range(epoch):\n",
    "    for ( batch, (images, labels)) in enumerate(dataset.take(6000)):\n",
    "        accuracy = tfe.metrics.Accuracy('accuracy', dtype=tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(images, training=True)\n",
    "            loss_value = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
    "            accuracy(tf.argmax(logits, axis=1, output_type=tf.int64),\n",
    "                    tf.cast(labels, tf.int64))\n",
    "\n",
    "        loss_history.append(loss_value.numpy())\n",
    "        acc_history.append(accuracy.result())\n",
    "        grads = tape.gradient(loss_value, model.variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables), \n",
    "                                 global_step=tf.train.get_or_create_global_step())\n",
    "        if batch % 20== 0 :\n",
    "            print(\"Loss at step {:03d}: {:.3f}, Accu : {:.3f}\".format(batch, loss_value, accuracy.result()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history)\n",
    "plt.plot(acc_history)\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('Loss [entropy]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
